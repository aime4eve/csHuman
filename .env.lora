# LoRA RAG 系统环境配置
# 复制此文件为 .env 并根据需要修改配置

# === 原有配置 ===
# 嵌入模型配置
OLLAMA_EMBEDDING_MODEL=quentinz/bge-large-zh-v1.5

# Ollama聊天模型配置（作为备用模型）
OLLAMA_CHAT_MODEL=qwen3:4b

# === LoRA 模型配置 ===
# 基础模型名称（用于LoRA微调的基础模型）
BASE_MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0

# LoRA适配器路径（微调后的LoRA权重文件夹）
LORA_MODEL_PATH=./lora_adapters

# 模型缓存目录（可选，用于存储下载的模型文件）
# 如果不设置，将使用HuggingFace默认缓存目录
CACHE_DIR=E:\LLM_Models

# 是否默认使用LoRA模型（true/false）
# 如果设置为false，将默认使用Ollama模型
USE_LORA_DEFAULT=true

# === 设备配置 ===
# 设备类型：auto, cuda, cpu
# auto: 自动检测CUDA可用性
# cuda: 强制使用CUDA（需要GPU支持）
# cpu: 强制使用CPU（默认配置，确保兼容性）
DEVICE=cpu

# === 推理配置 ===
# 最大生成长度
MAX_NEW_TOKENS=512

# 温度参数（控制生成的随机性，0.0-1.0）
TEMPERATURE=0.7

# Top-p采样参数
TOP_P=0.9

# 是否使用流式输出
STREAM_OUTPUT=false

# === 向量存储配置 ===
# 向量存储路径
VECTOR_STORE_PATH=./vector_store

# 检索时返回的文档数量
RETRIEVAL_K=3

# === 日志配置 ===
# 日志级别：DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# 是否启用详细日志
VERBOSE=false

# === API配置 ===
# API服务端口
API_PORT=8001

# API服务主机
API_HOST=127.0.0.1

# === 性能优化配置 ===
# 批处理大小
BATCH_SIZE=1

# 是否启用模型量化（可以减少内存使用）
ENABLE_QUANTIZATION=false

# 量化类型：int8, int4
QUANTIZATION_TYPE=int8

# === 安全配置 ===
# API密钥（可选，用于API访问控制）
# API_KEY=your_secret_key_here

# 允许的来源（CORS配置）
ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000