#!/usr/bin/env python3
"""
LoRA RAG ç³»ç»Ÿå¯åŠ¨è„šæœ¬

æ­¤è„šæœ¬ç”¨äºå¯åŠ¨é›†æˆäº†LoRAå¾®è°ƒæ¨¡å‹çš„RAGç³»ç»Ÿã€‚
æ”¯æŒè‡ªåŠ¨æ£€æµ‹LoRAæ¨¡å‹ã€é…ç½®éªŒè¯å’ŒæœåŠ¡å¯åŠ¨ã€‚

ä½¿ç”¨æ–¹æ³•:
    python start_lora_rag.py [é€‰é¡¹]

é€‰é¡¹:
    --port PORT         æŒ‡å®šAPIæœåŠ¡ç«¯å£ (é»˜è®¤: 8001)
    --host HOST         æŒ‡å®šAPIæœåŠ¡ä¸»æœº (é»˜è®¤: 127.0.0.1)
    --use-lora BOOL     æ˜¯å¦ä½¿ç”¨LoRAæ¨¡å‹ (é»˜è®¤: auto)
    --config-check      ä»…æ£€æŸ¥é…ç½®ï¼Œä¸å¯åŠ¨æœåŠ¡
    --verbose           å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
"""

import os
import sys
import argparse
import uvicorn
from pathlib import Path
from dotenv import load_dotenv

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    
    # æ£€æŸ¥.envæ–‡ä»¶
    env_files = [".env", ".env.lora"]
    env_loaded = False
    
    for env_file in env_files:
        if os.path.exists(env_file):
            load_dotenv(env_file)
            print(f"âœ… åŠ è½½ç¯å¢ƒé…ç½®: {env_file}")
            env_loaded = True
            break
    
    if not env_loaded:
        print("âš ï¸  æœªæ‰¾åˆ°ç¯å¢ƒé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        print("   å»ºè®®å¤åˆ¶ .env.lora ä¸º .env å¹¶æ ¹æ®éœ€è¦ä¿®æ”¹é…ç½®")
    
    return env_loaded

def check_lora_model():
    """æ£€æŸ¥LoRAæ¨¡å‹"""
    print("\nğŸ” æ£€æŸ¥LoRAæ¨¡å‹...")
    
    lora_path = os.getenv("LORA_MODEL_PATH", "./lora_adapters")
    lora_path = Path(lora_path)
    
    if lora_path.exists():
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶
        required_files = ["adapter_config.json", "adapter_model.safetensors"]
        missing_files = []
        
        for file in required_files:
            if not (lora_path / file).exists():
                missing_files.append(file)
        
        if missing_files:
            print(f"âš ï¸  LoRAæ¨¡å‹ç›®å½•å­˜åœ¨ä½†ç¼ºå°‘æ–‡ä»¶: {missing_files}")
            print(f"   è·¯å¾„: {lora_path.absolute()}")
            return False
        else:
            print(f"âœ… LoRAæ¨¡å‹æ£€æŸ¥é€šè¿‡: {lora_path.absolute()}")
            return True
    else:
        print(f"âŒ LoRAæ¨¡å‹ä¸å­˜åœ¨: {lora_path.absolute()}")
        print("   å°†ä½¿ç”¨Ollamaæ¨¡å‹ä½œä¸ºå¤‡ç”¨")
        return False

def check_base_model():
    """æ£€æŸ¥åŸºç¡€æ¨¡å‹é…ç½®"""
    print("\nğŸ” æ£€æŸ¥åŸºç¡€æ¨¡å‹é…ç½®...")
    
    base_model = os.getenv("BASE_MODEL_NAME", "Qwen/Qwen2.5-1.5B-Instruct")
    cache_dir = os.getenv("CACHE_DIR")
    
    print(f"ğŸ“¦ åŸºç¡€æ¨¡å‹: {base_model}")
    
    if cache_dir:
        cache_path = Path(cache_dir)
        if cache_path.exists():
            print(f"âœ… ç¼“å­˜ç›®å½•: {cache_path.absolute()}")
        else:
            print(f"âš ï¸  ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨åˆ›å»º: {cache_path.absolute()}")
    else:
        print("ğŸ“ ä½¿ç”¨é»˜è®¤HuggingFaceç¼“å­˜ç›®å½•")
    
    return True

def check_vector_store():
    """æ£€æŸ¥å‘é‡å­˜å‚¨"""
    print("\nğŸ” æ£€æŸ¥å‘é‡å­˜å‚¨...")
    
    vector_store_path = Path("./vector_store")
    
    if vector_store_path.exists() and any(vector_store_path.iterdir()):
        print(f"âœ… å‘é‡å­˜å‚¨å·²å°±ç»ª: {vector_store_path.absolute()}")
        return True
    else:
        print(f"âš ï¸  å‘é‡å­˜å‚¨ä¸ºç©ºæˆ–ä¸å­˜åœ¨: {vector_store_path.absolute()}")
        print("   è¯·ç¡®ä¿å·²ç»æ„å»ºäº†çŸ¥è¯†åº“å‘é‡ç´¢å¼•")
        return False

def check_ollama_service():
    """æ£€æŸ¥OllamaæœåŠ¡"""
    print("\nğŸ” æ£€æŸ¥OllamaæœåŠ¡...")
    
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸ï¼Œå¯ç”¨æ¨¡å‹æ•°é‡: {len(models)}")
            
            # æ£€æŸ¥é…ç½®çš„æ¨¡å‹æ˜¯å¦å­˜åœ¨
            embedding_model = os.getenv("OLLAMA_EMBEDDING_MODEL")
            chat_model = os.getenv("OLLAMA_CHAT_MODEL")
            
            model_names = [model.get("name", "") for model in models]
            
            if embedding_model and embedding_model in model_names:
                print(f"âœ… åµŒå…¥æ¨¡å‹å·²å®‰è£…: {embedding_model}")
            elif embedding_model:
                print(f"âš ï¸  åµŒå…¥æ¨¡å‹æœªå®‰è£…: {embedding_model}")
            
            if chat_model and chat_model in model_names:
                print(f"âœ… èŠå¤©æ¨¡å‹å·²å®‰è£…: {chat_model}")
            elif chat_model:
                print(f"âš ï¸  èŠå¤©æ¨¡å‹æœªå®‰è£…: {chat_model}")
            
            return True
        else:
            print(f"âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except ImportError:
        print("âš ï¸  requestsåº“æœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥OllamaæœåŠ¡")
        return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥OllamaæœåŠ¡: {e}")
        print("   è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ (http://localhost:11434)")
        return False

def check_dependencies():
    """æ£€æŸ¥Pythonä¾èµ–"""
    print("\nğŸ” æ£€æŸ¥Pythonä¾èµ–...")
    
    required_packages = [
        "fastapi", "uvicorn", "transformers", "torch", 
        "langchain", "langchain_community", "langchain_ollama",
        "peft", "accelerate", "python-dotenv"
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package}")
        except ImportError:
            missing_packages.append(package)
            print(f"âŒ {package}")
    
    if missing_packages:
        print(f"\nâš ï¸  ç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        print("   è¯·è¿è¡Œ: pip install " + " ".join(missing_packages))
        return False
    
    return True

def run_config_check():
    """è¿è¡Œå®Œæ•´çš„é…ç½®æ£€æŸ¥"""
    print("ğŸš€ LoRA RAG ç³»ç»Ÿé…ç½®æ£€æŸ¥\n")
    
    checks = [
        ("ç¯å¢ƒé…ç½®", check_environment),
        ("Pythonä¾èµ–", check_dependencies),
        ("LoRAæ¨¡å‹", check_lora_model),
        ("åŸºç¡€æ¨¡å‹", check_base_model),
        ("å‘é‡å­˜å‚¨", check_vector_store),
        ("OllamaæœåŠ¡", check_ollama_service)
    ]
    
    results = {}
    
    for name, check_func in checks:
        try:
            results[name] = check_func()
        except Exception as e:
            print(f"âŒ {name}æ£€æŸ¥å¤±è´¥: {e}")
            results[name] = False
    
    print("\n" + "="*50)
    print("ğŸ“‹ é…ç½®æ£€æŸ¥æ€»ç»“:")
    
    for name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {name}: {status}")
    
    all_passed = all(results.values())
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æ£€æŸ¥æœªé€šè¿‡ï¼Œç³»ç»Ÿå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œ")
        print("   è¯·æ ¹æ®ä¸Šè¿°æç¤ºè§£å†³é—®é¢˜åé‡è¯•")
    
    return all_passed

def start_server(host="127.0.0.1", port=8001, use_lora=None, verbose=False):
    """å¯åŠ¨LoRA RAGæœåŠ¡å™¨"""
    print(f"\nğŸš€ å¯åŠ¨LoRA RAGæœåŠ¡å™¨...")
    print(f"   ä¸»æœº: {host}")
    print(f"   ç«¯å£: {port}")
    
    if use_lora is not None:
        os.environ["USE_LORA_DEFAULT"] = str(use_lora).lower()
        print(f"   LoRAæ¨¡å¼: {'å¯ç”¨' if use_lora else 'ç¦ç”¨'}")
    
    if verbose:
        os.environ["LOG_LEVEL"] = "DEBUG"
        os.environ["VERBOSE"] = "true"
    
    try:
        # å¯¼å…¥åº”ç”¨
        from app.lora_main import app
        
        # å¯åŠ¨æœåŠ¡å™¨
        uvicorn.run(
            app,
            host=host,
            port=port,
            log_level="debug" if verbose else "info"
        )
    except ImportError as e:
        print(f"âŒ å¯¼å…¥åº”ç”¨å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸­è¿è¡Œæ­¤è„šæœ¬")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ å¯åŠ¨æœåŠ¡å™¨å¤±è´¥: {e}")
        sys.exit(1)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="LoRA RAG ç³»ç»Ÿå¯åŠ¨è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python start_lora_rag.py                    # ä½¿ç”¨é»˜è®¤é…ç½®å¯åŠ¨
  python start_lora_rag.py --port 8002       # æŒ‡å®šç«¯å£å¯åŠ¨
  python start_lora_rag.py --config-check    # ä»…æ£€æŸ¥é…ç½®
  python start_lora_rag.py --use-lora false  # ç¦ç”¨LoRAæ¨¡å‹
        """
    )
    
    parser.add_argument(
        "--port", 
        type=int, 
        default=8001,
        help="APIæœåŠ¡ç«¯å£ (é»˜è®¤: 8001)"
    )
    
    parser.add_argument(
        "--host", 
        type=str, 
        default="127.0.0.1",
        help="APIæœåŠ¡ä¸»æœº (é»˜è®¤: 127.0.0.1)"
    )
    
    parser.add_argument(
        "--use-lora", 
        type=str, 
        choices=["true", "false", "auto"],
        default="auto",
        help="æ˜¯å¦ä½¿ç”¨LoRAæ¨¡å‹ (é»˜è®¤: auto)"
    )
    
    parser.add_argument(
        "--config-check", 
        action="store_true",
        help="ä»…æ£€æŸ¥é…ç½®ï¼Œä¸å¯åŠ¨æœåŠ¡"
    )
    
    parser.add_argument(
        "--verbose", 
        action="store_true",
        help="å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®
    config_ok = run_config_check()
    
    if args.config_check:
        sys.exit(0 if config_ok else 1)
    
    if not config_ok:
        print("\nâš ï¸  é…ç½®æ£€æŸ¥æœªå®Œå…¨é€šè¿‡ï¼Œæ˜¯å¦ç»§ç»­å¯åŠ¨æœåŠ¡ï¼Ÿ (y/N): ", end="")
        response = input().strip().lower()
        if response not in ["y", "yes"]:
            print("å·²å–æ¶ˆå¯åŠ¨")
            sys.exit(1)
    
    # å¤„ç†use_loraå‚æ•°
    use_lora = None
    if args.use_lora == "true":
        use_lora = True
    elif args.use_lora == "false":
        use_lora = False
    # autoæ¨¡å¼ä¸‹use_loraä¿æŒNoneï¼Œç”±ç³»ç»Ÿè‡ªåŠ¨å†³å®š
    
    # å¯åŠ¨æœåŠ¡å™¨
    start_server(
        host=args.host,
        port=args.port,
        use_lora=use_lora,
        verbose=args.verbose
    )

if __name__ == "__main__":
    main()