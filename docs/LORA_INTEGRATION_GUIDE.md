# LoRA å¾®è°ƒæ¨¡å‹é›†æˆåˆ° RAG ç³»ç»ŸæŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•å°† LoRA å¾®è°ƒåçš„æ¨¡å‹é›†æˆåˆ°ç°æœ‰çš„ RAG (Retrieval-Augmented Generation) ç³»ç»Ÿä¸­ã€‚

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
3. [æ–‡ä»¶ç»“æ„](#æ–‡ä»¶ç»“æ„)
4. [é›†æˆæ­¥éª¤](#é›†æˆæ­¥éª¤)
5. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
6. [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
7. [API æ¥å£](#api-æ¥å£)
8. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
9. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
10. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## ğŸ¯ æ¦‚è¿°

### é›†æˆç›®æ ‡

- å°† LoRA å¾®è°ƒæ¨¡å‹æ— ç¼é›†æˆåˆ°ç°æœ‰ RAG ç³»ç»Ÿ
- æ”¯æŒåŠ¨æ€åˆ‡æ¢ LoRA æ¨¡å‹å’ŒåŸå§‹ Ollama æ¨¡å‹
- ä¿æŒå‘åå…¼å®¹æ€§ï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½
- æä¾›çµæ´»çš„é…ç½®é€‰é¡¹å’Œéƒ¨ç½²æ–¹å¼

### ä¸»è¦ç‰¹æ€§

- âœ… **åŒæ¨¡å‹æ”¯æŒ**: åŒæ—¶æ”¯æŒ LoRA å¾®è°ƒæ¨¡å‹å’Œ Ollama æ¨¡å‹
- âœ… **åŠ¨æ€åˆ‡æ¢**: è¿è¡Œæ—¶å¯åˆ‡æ¢ä¸åŒçš„æ¨¡å‹
- âœ… **è‡ªå®šä¹‰ç¼“å­˜**: æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ç¼“å­˜ç›®å½•
- âœ… **é…ç½®çµæ´»**: ä¸°å¯Œçš„ç¯å¢ƒå˜é‡é…ç½®é€‰é¡¹
- âœ… **å¥åº·æ£€æŸ¥**: å®Œæ•´çš„ç³»ç»ŸçŠ¶æ€ç›‘æ§
- âœ… **é”™è¯¯å¤„ç†**: ä¼˜é›…çš„é”™è¯¯å¤„ç†å’Œé™çº§æœºåˆ¶

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LoRA RAG ç³»ç»Ÿæ¶æ„                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  API Layer (FastAPI)                                       â”‚
â”‚  â”œâ”€â”€ /ask          - é—®ç­”æ¥å£                               â”‚
â”‚  â”œâ”€â”€ /switch_model - æ¨¡å‹åˆ‡æ¢                               â”‚
â”‚  â”œâ”€â”€ /model_info   - æ¨¡å‹ä¿¡æ¯                               â”‚
â”‚  â””â”€â”€ /health       - å¥åº·æ£€æŸ¥                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RAG Handler Layer                                         â”‚
â”‚  â”œâ”€â”€ LoRARAGHandler     - LoRA RAG å¤„ç†å™¨                  â”‚
â”‚  â”œâ”€â”€ LoRALanguageModel - LoRA è¯­è¨€æ¨¡å‹å°è£…                 â”‚
â”‚  â””â”€â”€ åŸå§‹ RAGHandler    - Ollama RAG å¤„ç†å™¨                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model Layer                                               â”‚
â”‚  â”œâ”€â”€ LoRA Model        - å¾®è°ƒåçš„ LoRA é€‚é…å™¨              â”‚
â”‚  â”œâ”€â”€ Base Model        - HuggingFace åŸºç¡€æ¨¡å‹              â”‚
â”‚  â””â”€â”€ Ollama Models     - Ollama æ‰˜ç®¡æ¨¡å‹                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Vector Store Layer                                        â”‚
â”‚  â”œâ”€â”€ FAISS Vector Store - å‘é‡æ•°æ®åº“                       â”‚
â”‚  â””â”€â”€ Embedding Model    - æ–‡æœ¬åµŒå…¥æ¨¡å‹                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
csHuman/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # åŸå§‹ RAG API
â”‚   â”œâ”€â”€ lora_main.py           # LoRA RAG API (æ–°å¢)
â”‚   â”œâ”€â”€ rag_handler.py         # åŸå§‹ RAG å¤„ç†å™¨
â”‚   â””â”€â”€ lora_rag_handler.py    # LoRA RAG å¤„ç†å™¨ (æ–°å¢)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ scheduler_config.json
â”œâ”€â”€ vector_store/              # å‘é‡å­˜å‚¨ç›®å½•
â”œâ”€â”€ lora_adapters/             # LoRA é€‚é…å™¨ç›®å½• (æ–°å¢)
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â””â”€â”€ ...
â”œâ”€â”€ .env                       # ç¯å¢ƒé…ç½®
â”œâ”€â”€ .env.lora                  # LoRA ç¯å¢ƒé…ç½®æ¨¡æ¿ (æ–°å¢)
â”œâ”€â”€ start_lora_rag.py         # LoRA RAG å¯åŠ¨è„šæœ¬ (æ–°å¢)
â”œâ”€â”€ LORA_INTEGRATION_GUIDE.md # æœ¬é›†æˆæŒ‡å— (æ–°å¢)
â””â”€â”€ requirements.txt
```

## ğŸ”§ é›†æˆæ­¥éª¤

### æ­¥éª¤ 1: å‡†å¤‡ LoRA æ¨¡å‹

1. **å®Œæˆ LoRA å¾®è°ƒ**:
   ```bash
   # ä½¿ç”¨å¾®è°ƒè„šæœ¬è®­ç»ƒæ¨¡å‹
   python finetune_lora.py \
     --model_name Qwen/Qwen2.5-1.5B-Instruct \
     --cache_dir E:\LLM_Models \
     --output_dir ./lora_adapters \
     --num_train_epochs 3
   ```

2. **éªŒè¯ LoRA è¾“å‡º**:
   ```bash
   # æ£€æŸ¥ LoRA é€‚é…å™¨æ–‡ä»¶
   ls -la ./lora_adapters/
   # åº”è¯¥åŒ…å«:
   # - adapter_config.json
   # - adapter_model.safetensors
   # - tokenizer ç›¸å…³æ–‡ä»¶
   ```

### æ­¥éª¤ 2: é…ç½®ç¯å¢ƒ

1. **å¤åˆ¶ç¯å¢ƒé…ç½®**:
   ```bash
   cp .env.lora .env
   ```

2. **ä¿®æ”¹é…ç½®æ–‡ä»¶**:
   ```bash
   # ç¼–è¾‘ .env æ–‡ä»¶
   nano .env
   ```

   å…³é”®é…ç½®é¡¹:
   ```env
   # LoRA æ¨¡å‹é…ç½®
   BASE_MODEL_NAME=Qwen/Qwen2.5-1.5B-Instruct
   LORA_MODEL_PATH=./lora_adapters
   CACHE_DIR=E:\LLM_Models
   USE_LORA_DEFAULT=true
   
   # è®¾å¤‡é…ç½®
   DEVICE=auto  # auto, cuda, cpu
   
   # API é…ç½®
   API_PORT=8001
   API_HOST=127.0.0.1
   ```

### æ­¥éª¤ 3: å®‰è£…ä¾èµ–

```bash
# å®‰è£… LoRA ç›¸å…³ä¾èµ–
pip install peft accelerate

# ç¡®ä¿å…¶ä»–ä¾èµ–å·²å®‰è£…
pip install fastapi uvicorn transformers torch langchain langchain_community langchain_ollama python-dotenv
```

### æ­¥éª¤ 4: éªŒè¯é›†æˆ

1. **è¿è¡Œé…ç½®æ£€æŸ¥**:
   ```bash
   python start_lora_rag.py --config-check
   ```

2. **å¯åŠ¨æœåŠ¡**:
   ```bash
   python start_lora_rag.py
   ```

3. **æµ‹è¯• API**:
   ```bash
   # æµ‹è¯•å¥åº·æ£€æŸ¥
   curl http://127.0.0.1:8001/health
   
   # æµ‹è¯•é—®ç­”
   curl -X POST http://127.0.0.1:8001/ask \
     -H "Content-Type: application/json" \
     -d '{"query": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", "use_lora": true}'
   ```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡è¯¦è§£

| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `BASE_MODEL_NAME` | `Qwen/Qwen2.5-1.5B-Instruct` | åŸºç¡€æ¨¡å‹åç§° |
| `LORA_MODEL_PATH` | `./lora_adapters` | LoRA é€‚é…å™¨è·¯å¾„ |
| `CACHE_DIR` | - | æ¨¡å‹ç¼“å­˜ç›®å½• |
| `USE_LORA_DEFAULT` | `true` | é»˜è®¤æ˜¯å¦ä½¿ç”¨ LoRA |
| `DEVICE` | `auto` | è®¡ç®—è®¾å¤‡ |
| `MAX_NEW_TOKENS` | `512` | æœ€å¤§ç”Ÿæˆé•¿åº¦ |
| `TEMPERATURE` | `0.7` | ç”Ÿæˆæ¸©åº¦ |
| `TOP_P` | `0.9` | Top-p é‡‡æ · |
| `API_PORT` | `8001` | API ç«¯å£ |
| `API_HOST` | `127.0.0.1` | API ä¸»æœº |

### è®¾å¤‡é…ç½®

ç³»ç»Ÿæ”¯æŒè‡ªåŠ¨æ£€æµ‹å’Œæ‰‹åŠ¨é…ç½®è®¡ç®—è®¾å¤‡ï¼Œ**é»˜è®¤é…ç½®ä¸ºCPUæ¨¡å¼ä»¥ç¡®ä¿æœ€å¤§å…¼å®¹æ€§**ï¼š

- `cpu`: å¼ºåˆ¶ä½¿ç”¨CPUï¼ˆ**é»˜è®¤é…ç½®ï¼Œç¡®ä¿å…¼å®¹æ€§**ï¼‰
- `auto`: è‡ªåŠ¨æ£€æµ‹ CUDA å¯ç”¨æ€§
- `cuda`: å¼ºåˆ¶ä½¿ç”¨ GPU (éœ€è¦ CUDA æ”¯æŒ)

**é‡è¦è¯´æ˜**ï¼šå¾®è°ƒåçš„æ¨ç†åº”ç”¨é»˜è®¤é…ç½®ä¸ºCPUæ¨¡å¼ï¼Œæ— éœ€GPUå³å¯è¿è¡Œã€‚å¦‚æœæ‚¨æœ‰GPUèµ„æºï¼Œå¯ä»¥å°† `DEVICE` è®¾ç½®ä¸º `auto` æˆ– `cuda` ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

### ç¼“å­˜ç›®å½•é…ç½®

å¦‚æœè®¾ç½®äº† `CACHE_DIR`ï¼Œæ¨¡å‹å°†ä¸‹è½½åˆ°æŒ‡å®šç›®å½•:
```
CACHE_DIR/
â””â”€â”€ models--Qwen--Qwen2.5-1.5B-Instruct/
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ ...
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³• 1: ä½¿ç”¨å¯åŠ¨è„šæœ¬ (æ¨è)

```bash
# åŸºæœ¬å¯åŠ¨
python start_lora_rag.py

# æŒ‡å®šç«¯å£
python start_lora_rag.py --port 8002

# ç¦ç”¨ LoRA æ¨¡å‹
python start_lora_rag.py --use-lora false

# å¯ç”¨è¯¦ç»†æ—¥å¿—
python start_lora_rag.py --verbose

# ä»…æ£€æŸ¥é…ç½®
python start_lora_rag.py --config-check
```

### æ–¹æ³• 2: ç›´æ¥å¯åŠ¨

```bash
# ä½¿ç”¨ uvicorn ç›´æ¥å¯åŠ¨
uvicorn app.lora_main:app --host 127.0.0.1 --port 8001

# æˆ–ä½¿ç”¨ Python
python -m app.lora_main
```

### æ–¹æ³• 3: åŒæ—¶è¿è¡Œä¸¤ä¸ªæœåŠ¡

```bash
# ç»ˆç«¯ 1: å¯åŠ¨åŸå§‹ RAG æœåŠ¡
uvicorn app.main:app --host 127.0.0.1 --port 8000

# ç»ˆç«¯ 2: å¯åŠ¨ LoRA RAG æœåŠ¡
python start_lora_rag.py --port 8001
```

## ğŸ”Œ API æ¥å£

### 1. é—®ç­”æ¥å£

**POST** `/ask`

```json
{
  "query": "ä½ çš„é—®é¢˜",
  "use_lora": true  // å¯é€‰ï¼Œæ˜¯å¦ä½¿ç”¨ LoRA æ¨¡å‹
}
```

**å“åº”**:
```json
{
  "answer": "å›ç­”å†…å®¹",
  "source_documents": [...],
  "model_used": "lora",
  "processing_time": 1.23
}
```

### 2. æ¨¡å‹åˆ‡æ¢æ¥å£

**POST** `/switch_model`

```json
{
  "use_lora": false  // åˆ‡æ¢åˆ° Ollama æ¨¡å‹
}
```

### 3. æ¨¡å‹ä¿¡æ¯æ¥å£

**GET** `/model_info`

**å“åº”**:
```json
{
  "base_model": "Qwen/Qwen2.5-1.5B-Instruct",
  "lora_model_path": "./lora_adapters",
  "lora_exists": true,
  "using_lora": true,
  "cache_dir": "E:\\LLM_Models",
  "vector_store_path": "./vector_store",
  "embedding_model": "quentinz/bge-large-zh-v1.5"
}
```

### 4. å¥åº·æ£€æŸ¥æ¥å£

**GET** `/health`

**å“åº”**:
```json
{
  "status": "healthy",
  "components": {
    "rag_handler": true,
    "vector_store": true,
    "lora_model": true,
    "ollama_service": true
  },
  "details": {
    "model_info": {
      "base_model": "Qwen/Qwen2.5-1.5B-Instruct",
      "using_lora": true
    }
  }
}
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. LoRA æ¨¡å‹åŠ è½½å¤±è´¥

**é”™è¯¯**: `FileNotFoundError: LoRA adapter not found`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ LoRA æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la ./lora_adapters/

# ç¡®è®¤é…ç½®æ­£ç¡®
echo $LORA_MODEL_PATH

# é‡æ–°è®­ç»ƒ LoRA æ¨¡å‹
python finetune_lora.py --output_dir ./lora_adapters
```

#### 2. CUDA å†…å­˜ä¸è¶³

**é”™è¯¯**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³• 1: ä½¿ç”¨ CPU
export DEVICE=cpu

# æ–¹æ³• 2: å¯ç”¨é‡åŒ–
export ENABLE_QUANTIZATION=true
export QUANTIZATION_TYPE=int8

# æ–¹æ³• 3: å‡å°‘æ‰¹å¤„ç†å¤§å°
export BATCH_SIZE=1
```

#### 3. æ¨¡å‹ä¸‹è½½å¤±è´¥

**é”™è¯¯**: `Connection timeout`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è®¾ç½® HuggingFace é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–ä½¿ç”¨ä»£ç†
export HTTP_PROXY=http://your-proxy:port
export HTTPS_PROXY=http://your-proxy:port
```

#### 4. Ollama æœåŠ¡è¿æ¥å¤±è´¥

**é”™è¯¯**: `Connection refused to localhost:11434`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:11434/api/tags

# å®‰è£…æ‰€éœ€æ¨¡å‹
ollama pull qwen3:4b
ollama pull quentinz/bge-large-zh-v1.5
```

#### 5. LoRA å¾®è°ƒè„šæœ¬è·¯å¾„é”™è¯¯

**é”™è¯¯**: `can't open file 'finetune_lora.py': No such file or directory`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
# è„šæœ¬å·²è‡ªåŠ¨ä¿®å¤ï¼Œä½¿ç”¨åŠ¨æ€è·¯å¾„è·å–
python scripts/finetune_with_custom_cache.py
```

#### 6. SFTTrainer API å…¼å®¹æ€§é—®é¢˜

**é”™è¯¯**: `SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç³»ç»Ÿå·²è‡ªåŠ¨åˆ‡æ¢åˆ°æ ‡å‡† Trainer
# ä½¿ç”¨ DataCollatorForLanguageModeling è¿›è¡Œæ•°æ®æ•´ç†
# æ— éœ€æ‰‹åŠ¨ä¿®æ”¹ï¼Œè„šæœ¬å·²è‡ªåŠ¨é€‚é…
```

#### 7. CPU ç¯å¢ƒä¸‹æ¢¯åº¦è®¡ç®—é”™è¯¯

**é”™è¯¯**: `element 0 of tensors does not require grad and does not have a grad_fn`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç³»ç»Ÿå·²è‡ªåŠ¨ç¦ç”¨ GPU ä¸“ç”¨åŠŸèƒ½
# --fp16 False
# --gradient_checkpointing False
# ç¡®ä¿ LoRA å‚æ•°æ­£ç¡®è®¾ç½®ä¸ºå¯è®­ç»ƒçŠ¶æ€
```

### æ—¥å¿—è°ƒè¯•

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python start_lora_rag.py --verbose

# æˆ–è®¾ç½®ç¯å¢ƒå˜é‡
export LOG_LEVEL=DEBUG
export VERBOSE=true
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é‡åŒ–

```env
# å¯ç”¨ INT8 é‡åŒ–
ENABLE_QUANTIZATION=true
QUANTIZATION_TYPE=int8
```

### 2. ç¼“å­˜ä¼˜åŒ–

```env
# ä½¿ç”¨ SSD ä½œä¸ºç¼“å­˜ç›®å½•
CACHE_DIR=/path/to/ssd/cache

# é¢„åŠ è½½æ¨¡å‹
preload_models=true
```

### 3. å¹¶å‘é…ç½®

```bash
# ä½¿ç”¨å¤šä¸ª worker
uvicorn app.lora_main:app --workers 4 --host 0.0.0.0 --port 8001
```

### 4. GPU ä¼˜åŒ–

```env
# å¯ç”¨æ··åˆç²¾åº¦
ENABLE_FP16=true

# è®¾ç½® GPU å†…å­˜åˆ†é…
CUDA_MEMORY_FRACTION=0.8
```

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ï¼Ÿ

**A**: æ¨èä½¿ç”¨ Docker å®¹å™¨åŒ–éƒ¨ç½²:

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8001

CMD ["python", "start_lora_rag.py", "--host", "0.0.0.0"]
```

### Q2: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„ LoRA æ¨¡å‹ï¼Ÿ

**A**: æœ‰ä¸¤ç§æ–¹å¼:

1. **è¿è¡Œæ—¶åˆ‡æ¢**: ä½¿ç”¨ `/switch_model` API
2. **é‡å¯åˆ‡æ¢**: ä¿®æ”¹ `LORA_MODEL_PATH` ç¯å¢ƒå˜é‡åé‡å¯æœåŠ¡

### Q3: å¦‚ä½•ç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼Ÿ

**A**: å¯ä»¥é›†æˆç›‘æ§å·¥å…·:

```python
# æ·»åŠ  Prometheus æŒ‡æ ‡
from prometheus_client import Counter, Histogram

request_count = Counter('rag_requests_total', 'Total RAG requests')
response_time = Histogram('rag_response_time_seconds', 'RAG response time')
```

### Q4: å¦‚ä½•å¤‡ä»½å’Œæ¢å¤æ¨¡å‹ï¼Ÿ

**A**: 

```bash
# å¤‡ä»½ LoRA é€‚é…å™¨
tar -czf lora_backup.tar.gz ./lora_adapters/

# å¤‡ä»½å‘é‡å­˜å‚¨
tar -czf vector_store_backup.tar.gz ./vector_store/

# æ¢å¤
tar -xzf lora_backup.tar.gz
tar -xzf vector_store_backup.tar.gz
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LoRA å¾®è°ƒæŒ‡å—](./custom_cache_directory.md)
- [åŸå§‹ RAG ç³»ç»Ÿæ–‡æ¡£](./README.md)
- [API æ¥å£æ–‡æ¡£](./API_DOCS.md)
- [éƒ¨ç½²æŒ‡å—](./DEPLOYMENT.md)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›è¿™ä¸ªé›†æˆæ–¹æ¡ˆï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚